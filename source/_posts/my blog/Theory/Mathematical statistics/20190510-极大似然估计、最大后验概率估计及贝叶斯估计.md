---
title: 贝叶斯估计、极大似然估计、最大后验概率估计
catalog: true
toc_nav_num: true
layout: post
mathjax: true
subtitle: 统计基本概念--贝叶斯公式及条件概率
tags: Bayes principles
categories:
  - Theory
  - Mathematical statistics
abbrlink: 5f4961b2
date: 2019-05-10 12:56:06
header-img:
---
## 频率学派与贝叶斯学派
频率学派与贝叶斯学派探讨「不确定性」这件事时的出发点与立足点不同。
1. 频率学派认为世界是**确定**的，试图直接为「事件」本身建模，即事件在多次独立重复试验中发生的频率趋于极限`$p$`那么这个极限就是该事件的概率。
他们认为*模型参数是确定形式的一个未知变量*，希望通过类似方程组的方式从数据中求解。频率学派在对总体参数进行点估计时常采用的两种方式：矩估计和**最大似然估计**(Maximum Likelihood Estimation,MLE)，其中最大似然估计应用广泛，在*大样本量下*估计较好。
$$
\theta_{M L E}=\operatorname{argmax}_{\theta} p(data | \theta)
 \tag{1.1}$$
1. 贝叶斯学派认为世界是**不确定**的，因获取的信息不同而异。从「观察者」角度出发，假设对事件有一个预先估计，通过获取的信息不断调整之前的预估计。同一件事情，由于观察者知识的不完备，所认为的事件状态也不同。
他们认为*模型参数服从某种潜在分布的随机变量*，希望从数据中推知该分布。对于数据的观测方式不同或者假设不同，那么推知的该参数也会因此而存在差异。这就是贝叶斯派视角下用来估计参数的常用方法-**最大后验概率估计**（Maximum A Posteriori Estimation，MAP），这种方法在先验假设比较靠谱的情况下效果显著，随着数据量的增加，先验假设对于模型参数的主导作用会逐渐削弱，真实的数据样例会占据有利地位。极端情况下，比如把先验假设去掉，或者假设先验满足均匀分布的话，那和极大似然估计就如出一辙了。
$$
\theta_{M A P}=\operatorname{argmax}_{\theta} p(data | \theta) p(\theta)
 \tag{1.2}$$
频率学派的代表是采用类条件概率密度进行最大似然估计；贝叶斯学派的代表是采用后验概率进行最大后验概率估计。两者都属于参数的点估计，MAP可看作对先验和MLE的折衷，数据量足够大时，两者趋于一致。  

贝叶斯估计是MAP的进一步扩展，贝叶斯估计同样假定模型参数是一个随机变量，但并不是直接估计出参数的某个特定值，而是**估计参数的分布**，这是贝叶斯估计与最大后验概率估计不同的地方。
> 三种方法对比
 1. 模型选择：①计算复杂度：MLE简单微分运算，贝叶斯估计需要复杂多重积分，较难理解 ②准确性：小样本时贝叶斯框架允许通过新的数据获得的似然更新成为新的先验、并再次获得一个新后验，迭代循环更新。其估计误差更小。  
 2. 参数化估计的局限：MLE依赖所假设的概率分布正确性，贝叶斯推理依赖先验信息的正确性。  




## Bayes theorem
### 条件概率
联合概率与边缘概率关系：
$$
\begin{array}{l}{P(A=a)=\sum_{b} P(A=a, B=b)} \\ {P(A=b)=\sum_{a} P(A=a, B=b)}\end{array}
$$
条件概率：
$${P(A | B)=\frac{P(A, B)}{P(B)}}$$ 
$$ {P(A, B)=P(B) * P(A | B)=P(A) * P(B | A)}$$
### 全概率公式
$$
P(B)=P\left(B | A_{1}\right) P\left(A_{1}\right)+P\left(B | A_{2}\right) P\left(A_{2}\right)+\ldots+P\left(B | A_{n}\right) P\left(A_{n}\right)=\sum_{i=1}^{n} P\left(B | A_{i}\right) P\left(A_{i}\right)
$$
可将复杂事件的概率求解转化为不同条件下简单事件的概率求和问题。
### 贝叶斯公式

$$
P(A | B)=\frac{P(B | A)}{P(B)} * P(A)
$$
由全概率公式、贝叶斯公式可得：
$$
P\left(A_{i} | B\right)=\frac{P\left(B | A_{i}\right) P\left(A_{i}\right)}{P(B)}=\frac{P\left(B | A_{i}\right) P\left(A_{i}\right)}{\sum_{i=1}^{n} P\left(B | A_{i}\right) P\left(A_{i}\right)}
$$
### 贝叶斯法则
> 贝叶斯定理的意义在于使我们能利用已有的知识或信念（通常称为先验的）帮助计算相关事件的概率。    
  
在贝叶斯统计中，**先验概率**分布是在没有获得某些新的信息前对`$P(\theta)$`的不确定性猜测进行量化，可量化为一个参数或一个潜在变量。其不仅依赖于主观经验估计，也依据已有经验知识进行推断。通常将先验概率乘以似然函数再归一化后，即得到参数的后验概率分布，后验概率分布是在给定数据下，`$P(\theta)$`的分布. 

转化成数据与参数的角度思考贝叶斯定理： 
$\theta$代表感兴趣的事件，为参数的集合，data代表所观察到的特定数据集合
$$
\underbrace{P(\theta | \text { data })}_{\text { Posterior probability } }=\underbrace{P(\theta)}_{\text { Prior probability } } \times \underbrace{\frac{{\overbrace{P(\text { data } | \theta)}^{\text { Likelihood function } } }} {\underbrace{P(\text { data })}_{\text { Standardized constant } } }  }_{\text { Standardized likelihood(updating prior)  } }=\underbrace{\frac{P\left(\text { data } |\theta_{i}\right) P\left(\theta_{i}\right)} {\sum_{i=1}^{n} P\left(\text { data } | \theta_{i}\right) P\left(\theta_{i}\right)} }_{\text { total class-conditional probability } }
 \tag{2}$$
  

- `$P(\theta_{i})$`为`$\theta_{i}$`的*先验概率*，代表我们根据经验或知识所相信的
- `$P(\text { data }| \theta_{i})$`,为在已知`$\theta_{i}`下`$\text { data }$`发生的类条件概率 
- `$P(\theta| \text { data })$`为`$\theta`的*后验概率*，即已知`$\text { data }$`下`$\theta`的条件概率，这里称似然度
- `$P(\text { data })$`为数据的先验概率  


**贝叶斯定理**：指在概率统计中利用所观察到的现象或新获得的信息对有关概率分布的先验概率进行修正的方法，当样本量充分大时，样本中事件发生的概率将接近于总体中事件发生概率。
> 频率派根据随机事件发生的频率赋值概率，贝叶斯派根据先验信息或背景知识赋值概率  


## Likelihood vs Probability
> Probability attaches to possible results; likelihood attaches to hypotheses.  
> The results to which probabilities attach are mutually exclusive and exhaustive; the hypotheses to which likelihoods attach are often neither; the range in one hypothesis may include the point in another.[^1]  


### 似然与概率
似然和概率同指事件发生的可能性，但在数理统计中含义不同，似然是贝叶斯统计的基石。
概率描述给定模型参数后，数据可能的结果，其概率之和为1.  
似然描述给定特定观测值(数据)后，参数为随机变量，似然之和可能大于1.
**抛硬币试验**[^2]
采用R的二项分布函数进行抛硬币试验，考虑参数：抛掷硬币次数，预测正面朝上的成功次数，预测正面朝上的成功的概率。
**概率**：假定服从二项分布，已知参数：抛掷硬币次数=10，预测正面朝上的成功的概率=0.5，预测正面朝上的成功次数为随机变量
``` R
barplot(
  dbinom(x = 0:10, size = 10, prob = 0.5),
  names.arg = 0:10,
  ylab="Probability",
  xlab="Number of successes"
)
```
![画出不同预测成功次数的概率图](http://wx3.sinaimg.cn/large/e775c73agy1g2wl3kuqnvj20dq0dkweo.jpg)
**似然**：已知参数：抛掷硬币次数=10，预测正面朝上的成功次数=8，预测正面朝上的成功的概率为随机变量
``` R
curve(
  dbinom(8,10,x), xlim = c(0,1),
  ylab="Likelihood",
  xlab=expression(paste("Binomial ", rho)),
)
```
![画出成功概率`$p$`的似然](http://wx1.sinaimg.cn/large/e775c73agy1g2wlfompg9j20de0c9q3a.jpg)
### 似然函数与概率函数 
对于
$$P(data | \theta)$$
 1. 如果`$\theta$`已知，`$data$`是变量，则称概率函数(probability function)，描述**不同样本**`$data$`出现的概率。
 2. 如果`$\theta$`为变量，`$data$`已知，则称似然函数(likelihood function), 描述对于**不同的模型参数**`$\theta$`下，`$data$`这个样本点出现的概率，也可记作`$L(\theta | data)$`或`$L(data ; \theta)$`或`$f(data ; \theta)$`。(似然函数的不同写法)
### 推理公式 

对离散函数 
 $$\mathcal{L}(\theta | d a t a)=P(d a t a | \theta)$$
对连续函数
 $$\mathcal{L}(\theta | d a t a)=f(d a t a | \theta)$$
> 1. 给定参数后数据的概率等于给定数据后参数的似然度。
> 2. 问题角度不同：一个是关于参数值：在给定data后，参数取特定值的似然度；一个关于数据：在给定参数条件下，观察到data的概率。   
> 
> **注**：1. 连续条件下似然函数不是概率密度函数，其积分和不为1，该似然函数指给定数据条件下参数值的概率，其与数据无关  
  
### 模型选择与参数估计：贝叶斯因子（Bayes factor）[^3]、[^4]
> 单个似然值无意义，似然是在比较中进行解释的   
 
${H}_{0}:\theta=\theta_{0}$  
    
${H}_{1}:\theta$具有先验分布$P(\theta | \mathcal{H}_{1})$。并可依据新data信息运用bayes原理更新$P(\mathcal{H}_{1})/P(\mathcal{H}_{0})$。  


在${H}_{0}$条件下，贝叶斯因子=似然比   
 
在${H}_{1}$条件下，相当于加权似然比，部分反映后验信息中，样本信息在三种信息（总体、先验、样本）中所起作用    

Bayes factor=1,代表样本信息无作用，先验信息准确
Bayes factor>1,支持`${H}_{1}$`，样本信息更新了先验信息
Bayes factor<1,支持`${H}_{0}$`，  

$$
\underbrace{\frac{P\left(\mathcal{H}_{1} | \text { data }\right)} {P\left(\mathcal{H}_{0} | \text { data }\right)} }_{\text { Posterior uncertainty } }=\underbrace{\frac{P\left(\mathcal{H}_{1}\right)} {P\left(\mathcal{H}_{0}\right)} }_{\text { Prior uncertainty } } \times \underbrace{\frac{P\left(\text { data } | \mathcal{H}_{1}\right)} {P\left(\text { data } | \mathcal{H}_{0}\right)} }_{\text { updating factor } }
$$ 




## 极大似然估计（MLE）
在实际问题中我们仅能获得有限样本数据，而先验概率和类条件概率未知。先验概率分布可依据经验知识或训练样本中各类的频率进行估计。而直接估计类条件概率的密度函数很难，<font color="#dd0000">极大似然估计将概率密度`$p(\text { data }| \theta_{i})$`转化为估计参数。</font><br />
**前提**：  
1. 样本分布代表其真实分布
2. 样本集`$data=\left\{x_{1}, x_{2}, x_{3}, \dots, x_{n}\right\}$`中样本`$x_{i}$`满足独立同分布的随机变量(iid条件)
3. 训练样本足够  
**目的**：利用已知样本结果，反推最大概率导致该结果的参数值。 ***模型已定，参数未知***  
**核心思想**：极大似然估计认为参数为固有的，但可能由于噪音干扰，存在误差。但只要在给定数据情况下，找到导致该结果概率最大的参数值即可，<font color="#dd0000">其实质为条件概率求最大解，即求使得`$P(\theta|data )$`最大的参数值`$\theta$`。</font><br />。
   
由贝叶斯公式可知
$$
P(\theta | d a t a) \propto P(d a t a | \theta) \times P(\theta)
$$
即可转化为
$$
\operatorname{argmax}_{\theta} P(\theta|data ){\Rightarrow}\operatorname{argmax}_{\theta} P(data | \theta)
 $$ 
### 最大似然估计模型推导
由全概率公式得
$$
\underbrace{l(\theta|data)=P(data | \theta)}_{\text {Likehood function}}=P\left(x_{1}, x_{2}, \cdots, x_{N} | \theta\right)=\prod_{i=1}^{N} p\left(x_{i} | \theta\right)
$$
式中的`$P(data | \theta)$`即为似然函数，因最终求解的为似然最大的参数值，所以称似然估计。
#### 对数似然函数(拉普拉斯修正)
为防止先验概率为0导致似然函数为0，以及多先验概率相乘可能出现的下溢出，引入拉普拉斯修正。
$$
{L}(\theta | data)=\ln \mathrm{L}(\theta | data)=\sum_{i=1}^{n} \ln p\left(x_{i} | \theta\right)
$$
即求解
$$
\hat{\theta}=\arg \max _{\theta} L(\theta | data)
$$
#### 最大似然估计量的一般步骤[^5]
1. 写出似然函数；
2. 对似然函数取对数，并整理；
3. 求导数
4. 解似然方程。
#### 最大似然估计参数估计的特点
1. 比其他估计方法简单
2. 收敛性：无偏或者渐近无偏，当样本数目增加时，收敛性质会更好
3. 如果假设的类条件概率模型正确，则通常能获得较好的结果。但如果假设模型出现偏差，将导致非常差的估计结果。
> 可能存在问题:在实践中对数似然函数的导数仍然是难以解析的，最大似然估计不一定能精确地得到解。一般采用期望最大化（EM）算法等迭代方法为参数估计找到数值解。
 
  
### 最小二乘参数估计 vs 最大似然估计 [^6]
当模型被假设为高斯分布时，MLE 的估计等价于最小二乘法。
**线性回归模型举例**
假设未知函数的参数形式为`$f_{0}(x)=f\left(x, \alpha_{0}\right)$`，`$\alpha_{0}$`为未知参数向量，存在已知密度函数`$p(\xi_{i})$`的可加噪音`$\xi_{i}~N(0,\sigma^{2})$`，对任意`$x_{i}$`的测量值为：
$$
y_{i}=f\left(x_{i}, \alpha_{0}\right)+\xi_{i}
$$
目的：从函数空间`$f\left(x, \alpha\right)$`中利用上述被噪音干扰的观察值估计`$f\left(x, \alpha_{0}\right)$`.
最大似然估计：
$$
L(\alpha)=\sum_{i=1}^{\ell} \ln p\left(y_{i}-f\left(x_{i}, \alpha\right)\right)
$$
而依据正态分布公式：
$$
p(\xi)=\frac{1}{\sigma \sqrt{2 \pi}} \exp \left\{-\frac{\xi^{2}}{2 \sigma^{2}}\right\}
$$
最小二乘法：
$$
L^{*}(\alpha)=-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{\ell}\left(y_{i}-f\left(x_{i}, \alpha\right)\right)^{2}-\ell \ln (\sqrt{2 \pi} \sigma)
$$
即最大化`$L^{*}(\alpha)$`等于最小化最小二乘估计函数

$$
M(\alpha)=\sum_{i=1}^{\ell}\left(y_{i}-f\left(x_{i}, \alpha\right)\right)^{2}
$$
## 最大后验估计(MAP)  
**目的**： 最大化在给定数据样本情况下模型参数的后验概率  
**核心思想**：可将MAP看作正则化的MLE，引入模型参数先验假设，不再仅依靠数据样本。当`$\theta$`为均匀分布时，MLE与MAP等价。  

相比MLE，MAP引入了先验概率`$P(\theta)$`：MLE从频率学派视角将$\theta$看作固定未知值，使得似然函数`$P(data | \theta)$`最大的参数即为最好的$\theta$；MAP从贝叶斯学派的观点出发，将$\theta$看作随机变量，其具有某种概率分布，即先验分布，认为应考虑$\theta$的先验分布`$P(\theta)$`，最大化函数为`$P(data | \theta)P(\theta)$`。由于$P(data)$可通过分析数据所得，则
$$
\underset{\theta}{\operatorname{argmax}} P(\theta | data)=\underset{\theta}{\operatorname{argmax}} \frac{P(data | \theta) P(\theta)}{P(data)} \propto \underset{\theta}{\operatorname{argmax}} P(data | \theta) P(\theta)=\underset{\theta}{\operatorname{argmax}} \sum_{i=1}^{n} \ln p\left(x_{i} | \theta\right)+ \ln p(\theta)
$$
#### 最大后验估计的一般步骤[^7]
1. 确定参数的先验分布以及似然函数；
2. 确定参数的后验分布函数
3. 将后验分布函数取对数，并整理；
4. 求导数
5. 求对数函数的最大值（求导，解方程）


## Reference
[^1]: [Bayes for Beginners: Probability and Likelihood](https://www.psychologicalscience.org/observer/bayes-for-beginners-probability-and-likelihood)
[^2]: [The difference between probability and likelihood using coins](https://acarril.github.io/posts/probability-likelihood)
[^3]: [Understanding Bayes: A Look at the Likelihood](https://alexanderetz.com/2015/04/15/understanding-bayes-a-look-at-the-likelihood/)
[^4]: [Bayes Factors for Those Who Hate Bayes Factors](https://www.bayesianspectacles.org/bayes-factors-for-those-who-hate-bayes-factors/)
[^5]: [极大似然估计详解](https://blog.csdn.net/zengxiantao1994/article/details/72787849)
[^6]: [The Nature of Statistical Learning Theory](https://book.douban.com/subject/1756954/)
[^7]: [贝叶斯估计、最大似然估计、最大后验概率估计](http://noahsnail.com/2018/05/17/2018-05-17-%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1%E3%80%81%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E3%80%81%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%E4%BC%B0%E8%AE%A1/)